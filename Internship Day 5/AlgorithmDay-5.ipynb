{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab6fedd-9958-4a91-b69b-525153401f40",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42ee400e-1682-4d22-9f02-bba1e04c122e",
   "metadata": {},
   "source": [
    "Linear Regression models the relationship between a dependent variable and one or more independent variables as a linear function. It's oneof the the simplest and most widely used regression techniques.\n",
    "\n",
    "y = mx + c\n",
    "\n",
    "y -> dependent variable\n",
    "x -> independent variable\n",
    "m -> represents the slope\n",
    "c -> intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbaff6-ca6b-43f2-aeb1-1198ef2d5fa0",
   "metadata": {},
   "source": [
    "# Predicting the house price based on size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec11b8-6b25-4c8f-a854-2f14d22263c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea3a0a-aa47-4fee-bc31-3d82b8fc2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data size of house (sq ft) and price \n",
    "x = np.array([1500, 1600, 1700, 1800, 1900]).reshape(-1, 1)\n",
    "y = np.array([300000, 320000, 340000, 360000, 380000])\n",
    "\n",
    "# model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# prediction\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# plot\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, color='red')\n",
    "plt.xlabel('Size (sq ft)')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b624e-7b97-4fa8-89a7-e99a465ec05c",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION: \n",
    "is used to predict binary outcomes (e.g: YES/NO, 1/0, Pass/Fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a966a-d203-46a3-954f-bee362f8925d",
   "metadata": {},
   "source": [
    "# Predicting whether a student will pass or fail based on study hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fd23b-8b7b-466b-bc31-222d15a9a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data: hours studied and pass/fail outcome\n",
    "X = np.array([5, 10, 15, 20, 25]).reshape(-1, 1)\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "y_predict = model.predict(X)\n",
    "\n",
    "# Display prediction\n",
    "print('Predicted values:', y_predict)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, y_predict, color='red', label='Logistic Regression')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Pass/Fail')\n",
    "plt.title('Logistic Regression')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e9daa-6ce4-4335-bc5b-7c4d305c22c2",
   "metadata": {},
   "source": [
    "# POLYNOMIAL REGRESSION\n",
    "Ploynomial regresssion is an extension of linear regression that allows for the modeling of relationships between the independent and dependent variables as an nth degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7ab67-7cb7-4c55-abcd-e4f0c5ecb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c8e11-0220-4945-92db-7e7a19bd8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (x -> houries studied, y -> test score)\n",
    "x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1, 4, 9, 16, 25])\n",
    "\n",
    "# transform features to include polynomial terms(degree 2)\n",
    "poly = PolynomialFeatures(degree = 2)\n",
    "x_poly = poly.fit_transform(x)\n",
    "\n",
    "# fit the polynomial regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)\n",
    "\n",
    "# predict values\n",
    "x_fit = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "x_fit_poly = poly.transform(x_fit)\n",
    "y_pred = model.predict(x_fit_poly)\n",
    "\n",
    "# plot results\n",
    "plt.scatter(x,  y, color='red', label='data points')\n",
    "plt.plot(x_fit, y_pred, color='blue', label='Polynomial fit')\n",
    "plt.xlabel('Hours studied')\n",
    "plt.ylabel('Test scores')\n",
    "plt.title('POLYNOMIAL REGRESSION (DEGREE 2)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92decc-f567-4648-90e2-90a754ac2a46",
   "metadata": {},
   "source": [
    "# DECISION TREE\n",
    "Decision tree classify the data by spliting the data into subsets based on the feature values, creating a tree-like model of decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88054fcd-55d1-4092-bc32-64b06979b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sample data (x -> feature, y -> labels)\n",
    "x = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Create and fit the model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predicting new data\n",
    "new_data = np.array([[2.5], [3.5]])\n",
    "predictions = model.predict(new_data)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fa4f5-15d9-44b4-b104-6e93b95dc740",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random Forest is an ensemble method that uses multiple decision tree to improve classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9d3d8-ab81-4eef-b045-77960c8a4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample data (x -> feature, y -> labels)\n",
    "x = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Create and fit the model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predicting new data\n",
    "new_data = np.array([[2.5], [3.5]])\n",
    "predictions = model.predict(new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c4d59-bcd7-4157-bc06-695ff737e661",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machine)\n",
    "SVM is used for classification by finding the hyperplane that best seperates the classes in feature space.\n",
    "1. SVC : Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94550823-aa9a-41f6-828a-64569e04099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sample data (x -> feature, y -> labels)\n",
    "x = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Create and fit the model\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predicting new data\n",
    "new_data = np.array([[2.5], [3.5]])\n",
    "predictions = model.predict(new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd172e3-ddec-428a-a42e-c168fc0fec0d",
   "metadata": {},
   "source": [
    "# KNN(Kth-NEAREST NEIGHNOUR) CLASSIFIER\n",
    "KNN is a non-parametric method used for classification by finding the majority class among the k-nearest neighnours of a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74668d-21c7-46f8-9395-0bfc0542747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Sample data (x -> feature, y -> labels)\n",
    "x = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Create and fit the model\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predicting new data\n",
    "new_data = np.array([[2.5], [3.5]])\n",
    "predictions = model.predict(new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80bbf14-bdbe-4bc8-856b-aa107cc06258",
   "metadata": {},
   "source": [
    "# NAIVE BAYES\n",
    "Probablistic ML Algorithm based on Bayes Theorm\n",
    "The formula for Bayes’ Theorem is:\n",
    "# P(A∣B) = ​P(B|A) * P(A) / P(B)\n",
    "Where:\n",
    "1. ( P(A|B) ) is the probability of event ( A ) occurring given that ( B ) is true\n",
    "2. \n",
    "( P(B|A) ) is the probability of event ( B ) occurring given that ( A ) is \n",
    "reu\n",
    "3. \r\n",
    "( P(A) ) is the probability of event ( A ) occuaneinrr\n",
    "4. .\r\n",
    "( P(B) ) is the probability of event ( B ) occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9366bd3-c1ad-4fa5-a207-f93036e08ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predicting new data\n",
    "new_data = np.array([[2, 2], [4, 4]])\n",
    "predictions = model.predict(new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b6f34-c382-441e-9a6b-03a6309d6330",
   "metadata": {},
   "source": [
    "# ADABOOST ALGORITHM (ADAPTIVE BOOST)\n",
    "AdaBoost, or Adaptive Boosting, is an ensemble learning technique that combines multiple weak classifiers to create a strong classifier. Here’s a detailed explanation and a Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c484c5-ac3c-4a0b-9de3-df32ed3a392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b8cac-41a2-4486-a96b-9501cbc9b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the AdaBoost model\n",
    "base_model = DecisionTreeClassifier(max_depth=1)\n",
    "model = AdaBoostClassifier(estimator=base_model, n_estimators=50)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(x_test)\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72963701-a4f0-4582-87b6-18fdb87fe4ee",
   "metadata": {},
   "source": [
    "# XGBOOST ALGORITHM\n",
    "XGBoost (eXtreme Gradient Boosting) is a powerful and efficient implementation of the gradient boosting framework.\n",
    "xgb : eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2366228-0afb-45b6-beeb-9d9a238031a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19967f-19bf-46a6-b123-089715a7f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "x, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "train_dmatrix = xgb.DMatrix(x_train, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the model\n",
    "params = {'objective': 'multi:softmax', 'num_class': 3, 'eval_metric': 'mlogloss'}\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(params, train_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(test_dmatrix)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca2c54-2499-4c79-a2f4-f8e0c374ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
